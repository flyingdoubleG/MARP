import litellm
from pathlib import Path
from tqdm import tqdm
import numpy as np

GENERATE_ESSAY_PROMPT_TEMPLATE = "Based on premise: \"{}\" generate story containing several scenes, use scene1:, scene2:, ... to represent."

# RATE_ESSAY_PROMPT_TEMPLATE="Based on 1. Interesting. Interesting to the reader. 2. Coherent. Plot-coherent. 3. Relevant. Faithful to the initial premise. 4. Humanlike. Judged to be human-written.4 dimensions evaluate following 2 stories, the score is from 0 to 100, higher score means better.\nThe initial premise of story is \"{}\"\nStory 1: {}\n Story 2: {}."

RATE_ESSAY_PROMPT_TEMPLATE="Based on the following six categories: 1. Relevance. 2. Coherence. 3. Empathy. 4. Surprise. 5. Engagement. 6. Complexity, evaluate the following two stories by assigning an integer score (from 1 to 5) to each category. Higher score means better.\nThe initial premise of story is: {}\nStory1: {}\n Story2: {}.\n\nIn your response, please use the following example format: \nStory1\nRelevance: 3\nCoherence: 4\nEmpathy: 5\nSurprise: 2\nEngagement: 4\nComplexity: 3\nStory2\nRelevance: 3\nCoherence: 4\nEmpathy: 5\nSurprise: 2\nEngagement: 4\nComplexity: 3\n"


QUARREL_PREMISE = "You will collaborate to create a story. The general setting: A Quarrel between two good friends about Iron Man."
IBRUSIA_PREMISE = "You will collaborate to create a story. The general setting: The state of Ibrusia is coming to a desperate and dangerous situation as the Hosso Union approaches its capital, Zaragoza."
ECONOMY_PREMISE = "You will collaborate to create a story. The general setting: The state of Gurata is coming to a huge economic recession. People are in panic and streets are in turmoil."


def get_response(model, message):
    """
    Query the LLM model with a message and return the response.
    """
    response = litellm.completion(
        model=model,
        messages=[{"content": message, "role": "user"}],
    )
    return response.choices[0].message.content


def generate_baseline(baseline_model, premise, baseline_save_path):
    baseline = get_response(baseline_model, GENERATE_ESSAY_PROMPT_TEMPLATE.format(premise))
    with open(baseline_save_path, 'w+') as f:
        f.write(baseline)


def evaluate(essay_path, evaluator_model, premise, num_trials, baseline_path):
    with open(essay_path, 'r') as f:
        ours = f.read()
    with open(baseline_path, 'r') as f:
        baseline = f.read()
    evaluations = []
    for _ in tqdm(range(num_trials)):
        prompt = RATE_ESSAY_PROMPT_TEMPLATE.format(premise, ours, baseline)
        # if ('gpt-3.5' in evaluator_model or 'lama' in evaluator_model) and len(prompt) > 4096:
        #     print(f'truncating to 4096 tokens from {len(prompt)} tokens for {evaluator_model}')
        #     prompt = prompt[:4096]
        #     print(prompt)
        evaluation = get_response(evaluator_model, prompt)
        evaluations.append(evaluation)
    return evaluations


baseline_path = 'gemini-ibrusia.txt'
# generate_baseline('gemini-pro', IBRUSIA_PREMISE, baseline_path)
for evaluator_model in ['gpt-3.5-turbo-16k', 'gemini-pro', 'anyscale/mistralai/Mistral-7B-Instruct-v0.1']:
    print(f'Evaluating with {evaluator_model}...')
    evaluations = evaluate(
        essay_path='storys/mid_ibrusia.txt',
        evaluator_model=evaluator_model,
        premise=IBRUSIA_PREMISE,
        num_trials=5,
        baseline_path=baseline_path,
    )
    output_path = Path(f'evaluations/{evaluator_model}.txt')
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w+') as f:
        f.write(f'********{evaluator_model}********\n')
        f.write('\n\n\n********************************\n\n\n'.join(evaluations))


def parse_scores(response, num_categories=6):
    """
    Parses the scores from the evaluator's response.
    """
    try:
        # Splitting the response by newlines
        lines = response.strip().split('\n')
        
        # Placeholder for scores
        llmScore1, llmScore2 = [], []

        # Variable to keep track of which story's scores we are currently parsing
        current_story = 1

        for line in lines:
            if line.startswith("Story1"):
                continue
            elif line.startswith("Story2"):
                current_story = 2
            else:
                score = int(line.split(': ')[1])
                if current_story == 1:
                    llmScore1.append(score)
                else:
                    llmScore2.append(score)

        if len(llmScore1) != num_categories or len(llmScore2) != num_categories:
            raise ValueError("Incorrect number of scores for one or both stories")

        return llmScore1, llmScore2

    except Exception as e:
        # Handling any potential errors
        print(f"Error parsing scores: {e}")
        return None, None


def evaluate_stories(model, premises, stories1, stories2, scores1, scores2):
    """
    Evaluates two lists of stories generated by two different entities based on given premises.
    :param model: model to evaluate
    :param premises: list of premises
    :param stories1: list of articles generated by entity1
    :param stories2: list of articles generated by entity2
    :param scores1: list of scores for stories1. Each score is a list of integers for each category.
    :param scores2: list of scores for stories2. Each score is a list of integers for each category.
    """
    assert len(premises) == len(stories1) == len(stories2)
    assert len(scores1) == len(scores2)
    for i in range(len(scores1)):
        assert scores1[i] == scores2[i]
    
    llmScores1, llmScores2 = [], []
    
    for i in range(len(stories1)):
        premise = premises[i]
        story1 = stories1[i]
        story2 = stories2[i]
        score1 = scores1[i]
        score2 = scores2[i]

        prompt = RATE_ESSAY_PROMPT_TEMPLATE.format(premise, story1, story2)
        response = get_response(model, prompt)
        llmScore1, llmScore2 = parse_scores(response)
        
        if llmScore1 is None or llmScore2 is None:
            raise Exception(f"Error evaluating stories for premise {i+1}")
        else:
            llmScores1.append(llmScore1)
            llmScores2.append(llmScore2)
    return llmScores1, llmScores2


def compute_model_eval_acc(scores1, scores2, llmScores1, llmScores2):
    """
    Computes the model evaluation accuracy for each model.
    """
    assert len(scores1) == len(scores2) == len(llmScores1) == len(llmScores2)
    assert len(scores1[0]) == len(scores2[0]) == len(llmScores1[0]) == len(llmScores2[0])

    scores1 = np.array(scores1)
    scores2 = np.array(scores2)
    llmScores1 = np.array(llmScores1)
    llmScores2 = np.array(llmScores2)
    scores1 = scores1.sum(axis=1)
    scores2 = scores2.sum(axis=1)
    llmScores1 = llmScores1.sum(axis=1)
    llmScores2 = llmScores2.sum(axis=1)

    acc = 0
    
    for i in range(len(scores1)):
        if scores1[i] > scores2[i]:
            if llmScores1[i] > llmScores2[i]:
                acc += 1
        elif scores1[i] < scores2[i]:
            if llmScores1[i] < llmScores2[i]:
                acc += 1
        else:
            if llmScores1[i] == llmScores2[i]:
                acc += 1
    return acc / len(scores1)

        
if __name__ == '__main__':
    pass
