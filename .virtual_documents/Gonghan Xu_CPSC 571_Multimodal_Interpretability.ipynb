





!pip install -q transformers





from google.colab import drive
drive.mount('/content/drive/')


# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip


# !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip


# !wget http://images.cocodataset.org/zips/val2014.zip


# !ls


# !unzip v2_Annotations_Val_mscoco.zip


# !unzip v2_Questions_Val_mscoco.zip


# !unzip val2014.zip


# !find val2014 -maxdepth 1 -iname "*.jpg" | wc -l









import json

# Opening JSON file
# f = open('/content/drive/MyDrive/ViLT/Datasets/VQAv2/v2_OpenEnded_mscoco_val2014_questions.json')
f = open('v2_OpenEnded_mscoco_val2014_questions.json')

# Return JSON object as dictionary
data_questions = json.load(f)
print(data_questions.keys())





questions = data_questions['questions']
print("Number of questions:", len(questions))





questions[0]





import re
from typing import Optional

filename_re = re.compile(r".*(\d{12})\.((jpg)|(png))")

def id_from_filename(filename: str) -> Optional[int]:
    match = filename_re.fullmatch(filename)
    if match is None:
        return None
    return int(match.group(1))


from os import listdir
from os.path import isfile, join
from tqdm.auto import tqdm

# root at which all images are stored
# root = '/content/drive/MyDrive/ViLT/Datasets/VQAv2/val2014'
root = 'val2014'
file_names = [f for f in tqdm(listdir(root)) if isfile(join(root, f))]





id_from_filename('COCO_val2014_000000501080.jpg')





filename_to_id = {root + "/" + file: id_from_filename(file) for file in file_names}
id_to_filename = {v:k for k,v in filename_to_id.items()}





from PIL import Image

print(f"Size of map: {len(id_to_filename)}")

path = id_to_filename[questions[0]['image_id']]
image = Image.open(path)
image





import json

# Read annotations
# f = open('/content/drive/MyDrive/ViLT/Datasets/VQAv2/v2_mscoco_val2014_annotations.json')
f = open('v2_mscoco_val2014_annotations.json')

# Return JSON object as dictionary
data_annotations = json.load(f)
print(data_annotations.keys())





annotations = data_annotations['annotations']


print("Number of annotations:", len(annotations))





annotations[0]





from transformers import ViltConfig

config = ViltConfig.from_pretrained("dandelin/vilt-b32-finetuned-vqa")


from tqdm.notebook import tqdm

def get_score(count: int) -> float:
    return min(1.0, count / 3)

for annotation in tqdm(annotations):
    answers = annotation['answers']
    answer_count = {}
    for answer in answers:
        answer_ = answer["answer"]
        answer_count[answer_] = answer_count.get(answer_, 0) + 1
    labels = []
    scores = []
    for answer in answer_count:
        if answer not in list(config.label2id.keys()):
            continue
        labels.append(config.label2id[answer])
        score = get_score(answer_count[answer])
        scores.append(score)
    annotation['labels'] = labels
    annotation['scores'] = scores





annotations[0]





labels = annotations[0]['labels']
print([config.id2label[label] for label in labels])


scores = annotations[0]['scores']
print(scores)





import torch
from PIL import Image

class VQADataset(torch.utils.data.Dataset):
    """VQA (v2) dataset."""

    def __init__(self, questions, annotations, processor):
        self.questions = questions
        self.annotations = annotations
        self.processor = processor

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        # get image + text
        annotation = self.annotations[idx]
        questions = self.questions[idx]
        image = Image.open(id_to_filename[annotation['image_id']])
        text = questions['question']

        encoding = self.processor(image, text, padding="max_length", truncation=True, return_tensors="pt")
        # remove batch dimension
        for k,v in encoding.items():
          encoding[k] = v.squeeze()
        # add labels
        labels = annotation['labels']
        scores = annotation['scores']
        targets = torch.zeros(len(config.id2label))
        for label, score in zip(labels, scores):
              targets[label] = score
        encoding["labels"] = targets

        return encoding


from transformers import ViltProcessor

processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")

dataset = VQADataset(questions=questions[:100],
                     annotations=annotations[:100],
                     processor=processor)


dataset[0].keys()


processor.decode(dataset[0]['input_ids'])


labels = torch.nonzero(dataset[0]['labels']).squeeze().tolist()


[config.id2label[label] for label in labels]





from transformers import ViltForQuestionAnswering

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-mlm",
                                                 id2label=config.id2label,
                                                 label2id=config.label2id)
model.to(device)





from torch.utils.data import DataLoader

def collate_fn(batch):
  input_ids = [item['input_ids'] for item in batch]
  pixel_values = [item['pixel_values'] for item in batch]
  attention_mask = [item['attention_mask'] for item in batch]
  token_type_ids = [item['token_type_ids'] for item in batch]
  labels = [item['labels'] for item in batch]

  # create padded pixel values and corresponding pixel mask
  encoding = processor.image_processor.pad(pixel_values, return_tensors="pt")

  # create new batch
  batch = {}
  batch['input_ids'] = torch.stack(input_ids)
  batch['attention_mask'] = torch.stack(attention_mask)
  batch['token_type_ids'] = torch.stack(token_type_ids)
  batch['pixel_values'] = encoding['pixel_values']
  batch['pixel_mask'] = encoding['pixel_mask']
  batch['labels'] = torch.stack(labels)

  return batch

train_dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4, shuffle=True)








batch = next(iter(train_dataloader))


for k,v in batch.items():
  print(k, v.shape)





from PIL import Image
import numpy as np

image_mean = processor.image_processor.image_mean
image_std = processor.image_processor.image_std

batch_idx = 1

unnormalized_image = (batch["pixel_values"][batch_idx].numpy() * np.array(image_mean)[:, None, None]) + np.array(image_std)[:, None, None]
unnormalized_image = (unnormalized_image * 255).astype(np.uint8)
unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)
Image.fromarray(unnormalized_image)


processor.decode(batch["input_ids"][batch_idx])


labels = torch.nonzero(batch['labels'][batch_idx]).squeeze().tolist()


[config.id2label[label] for label in labels]





optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)

model.train()
for epoch in range(25):  # loop over the dataset multiple times
   print(f"Epoch: {epoch}")
   for batch in tqdm(train_dataloader):
        # get the inputs;
        batch = {k:v.to(device) for k,v in batch.items()}

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(**batch)
        loss = outputs.loss
        print("Loss:", loss.item())
        loss.backward()
        optimizer.step()





example = dataset[0]
print(example.keys())


processor.decode(example['input_ids'])


# add batch dimension + move to GPU
example = {k: v.unsqueeze(0).to(device) for k,v in example.items()}

# forward pass
outputs = model(**example)





logits = outputs.logits
predicted_classes = torch.sigmoid(logits)

probs, classes = torch.topk(predicted_classes, 5)
probs
for prob, class_idx in zip(probs.squeeze().tolist(), classes.squeeze().tolist()):
  print(prob, model.config.id2label[class_idx])








!pip install captum


import copy, os, sys

# Replace <PROJECT-DIR> placeholder with your project directory path
PROJECT_DIR = '/content'


import threading
import numpy as np

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn.functional as F


import matplotlib.pyplot as plt
from PIL import Image
from matplotlib.colors import LinearSegmentedColormap

from captum.attr import (
    IntegratedGradients,
    LayerIntegratedGradients,
    TokenReferenceBase,
    configure_interpretable_embedding_layer,
    remove_interpretable_embedding_layer,
    visualization
)
from captum.attr._utils.input_layer_wrapper import ModelInputWrapper


model.eval()





# !wget https://github.com/pytorch/captum/blob/master/tutorials/img/vqa/elephant.jpg
# !wget https://github.com/pytorch/captum/blob/master/tutorials/img/vqa/siamese.jpg
# !wget https://github.com/pytorch/captum/blob/master/tutorials/img/vqa/zebra.jpg











class ViltModelWrapper(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, pixel_values, attention_mask=None, pixel_mask=None, token_type_ids=None):
        # Construct the dictionary expected by the original model's forward method
        inputs = {
            "input_ids": input_ids,
            "pixel_values": pixel_values,
            "attention_mask": attention_mask,
            "pixel_mask": pixel_mask,
            "token_type_ids": token_type_ids,
        }
        # Filter out None values
        inputs = {k: v for k, v in inputs.items() if v is not None}
        outputs = self.model(**inputs)
        # Return logits directly
        return outputs.logits

# model wrapper to handle dictionary args
wrap_model = ViltModelWrapper(model)

# wrap the inputs into layers for attribution
m = ModelInputWrapper(wrap_model)





# TODO: create layer integrated gradients object
attr = LayerIntegratedGradients(m, [model.vilt.embeddings.patch_embeddings, model.vilt.embeddings.text_embeddings])
# attr = LayerIntegratedGradients(m, [m.input_maps["pixel_values"], m.input_maps["input_ids"]])





default_cmap = LinearSegmentedColormap.from_list('custom blue',
                                                 [(0, '#ffffff'),
                                                  (0.25, '#252b36'),
                                                  (1, '#000000')], N=256)





images = ['/content/drive/MyDrive/img/vqa/siamese.jpg',
          '/content/drive/MyDrive/img/vqa/elephant.jpg',
          '/content/drive/MyDrive/img/vqa/zebra.jpg']


def vilt_interpret(image_filename, questions, targets):
    img = Image.open(image_filename)

    for question, target in zip(questions, targets):
        inputs = processor(img, question, padding="max_length", truncation=True, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}

        additional_args = (
            inputs.get('attention_mask').to(device),
            inputs.get('pixel_mask').to(device),
            inputs.get('token_type_ids').to(device)
        )

        image_baseline = torch.zeros_like(inputs["pixel_values"])
        text_baseline = torch.full_like(inputs["input_ids"], processor.tokenizer.pad_token_id)

        logits = m(**inputs)
        predictions = torch.sigmoid(logits)
        probs, classes = predictions.topk(1, dim=1)
        predicted_class = classes.item()
        predicted_prob = probs.item()
        true_class = config.label2id[target]

        attributions = attr.attribute(inputs=(inputs["input_ids"], inputs["pixel_values"]),
                                      baselines=(text_baseline, image_baseline),
                                      target=true_class,   # Use true class as target
                                      additional_forward_args=additional_args,
                                      n_steps=30)

        # Normalize text attributions.
        text_attributions = attributions[1].sum(dim=2).squeeze(0)
        text_attributions_norm = text_attributions / text_attributions.norm()
        tokens = processor.tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

        # Visualize text attributions
        vis_data_record = visualization.VisualizationDataRecord(
            word_attributions=text_attributions_norm.tolist(),
            pred_prob=predicted_prob,
            pred_class=config.id2label[predicted_class],
            true_class=config.id2label[true_class],  # Adjust if you have the true class available
            attr_class=target,  # The class for which attributions were computed
            attr_score=attributions[1].sum().item(),
            raw_input_ids=tokens,
            convergence_score=0.0
        )
        visualization.visualize_text([vis_data_record])

        # Visualize image attributions
        original_im_mat = np.transpose(inputs['pixel_values'].squeeze(0).cpu().detach().numpy(), (1, 2, 0))

        # Get mean and std for image processor
        image_mean = np.array(processor.image_processor.image_mean)
        image_std = np.array(processor.image_processor.image_std)

        # Denormalize using the provided mean and std
        original_im_mat = (original_im_mat * image_std) + image_mean  # Apply denormalization
        original_im_mat = np.clip(original_im_mat, 0, 1)  # Ensure values are in [0, 1] range
        original_im_mat = (original_im_mat * 255).astype(np.uint8)  # Scale to [0, 255] range and convert to integers

        # Reshape attributions tensor
        attributions_img = np.transpose(attributions[0].squeeze(0).cpu().detach().numpy(), (1, 2, 0))

        visualization.visualize_image_attr_multiple(attributions_img, original_im_mat,
                                                    ["original_image", "heat_map"], ["all", "absolute_value"],
                                                    titles=["Original Image", "Attribution Magnitude"],
                                                    cmap=default_cmap,
                                                    show_colorbar=True)

        print('Image Contributions: ', attributions[0].sum().item())
        print('Text Contributions: ', attributions[1].sum().item())
        print('Total Contribution: ', attributions[0].sum().item() + attributions[1].sum().item())








# the index of image in the test set. Please, change it if you want to play with different test images/samples.
image_idx = 1 # elephant
vilt_interpret(images[image_idx], [
    "what is on the picture",
    "what color is the elephant",
    "where is the elephant"
], ['elephant', 'gray', 'zoo'])


image_idx = 0 # cat

vilt_interpret(images[image_idx], [
    "what is on the picture",
    "what color are the cat's eyes",
    "is the animal in the picture a cat or a fox",
    "what color is the cat",
    "how many ears does the cat have",
], ['cat', 'blue', 'cat', 'white and brown', '2'])


image_idx = 2 # zebra

vilt_interpret(images[image_idx], [
    "what is on the picture",
    "what color are the zebras",
    "how many zebras are on the picture",
    "where are the zebras"
], ['zebra', 'black and white', '2', 'zoo'])
